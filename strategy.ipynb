{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network for Predicting Stock Price Movements using State\n",
    "\n",
    "This example uses a **Long Short Term Memory (LSTM) Neural Network** to predict if stock prices will rise or fall. We trade the top assets with the lowest volatility and use previous weights to determine positions for the next day.\n",
    "\n",
    "**Important!** *Before further development, you need to run the ./init.py file once to install the PyTorch dependency.*\n",
    "```shell\n",
    "!pip install torch==2.2.1\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Universe:** NASDAQ-100 stocks\n",
    "- **Trading Logic:** Long positions are taken based on the confidence level of the LSTM modelâ€™s predictions.\n",
    "- **Feature for Learning:** Trend calculated using the rate of change (ROC) of the logarithm of the closing price with a linear weighted moving average (LWMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import qnt.data as qndata\n",
    "import qnt.backtester as qnbt\n",
    "import qnt.ta as qnta\n",
    "import qnt.stats as qns\n",
    "import qnt.graph as qngraph\n",
    "import qnt.output as qnout\n",
    "import qnt.filter as qnfilter\n",
    "import qnt.exposure as qnexp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "\n",
    "global_lookback_period = 155\n",
    "global_train_period = 100\n",
    "global_count_features_for_ml = 1\n",
    "global_top_assets = 12\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to define our LSTM network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=3, hidden_layers=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lstm1 = nn.LSTMCell(input_dim, self.hidden_layers)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "\n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        n_samples = y.size(0)\n",
    "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "\n",
    "        for time_step in range(y.size(1)):\n",
    "            x_t = y[:, time_step, :]  # Ensure x_t is [batch, input_dim]\n",
    "\n",
    "            h_t, c_t = self.lstm1(x_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1).squeeze(-1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    def set_seed(seed_value=42):\n",
    "        \"\"\"Set seed for reproducibility.\"\"\"\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    set_seed(42)\n",
    "    model = LSTM(input_dim=global_count_features_for_ml, hidden_layers=34)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_features(data):\n",
    "    close_price = data.sel(field=\"close\").ffill('time').bfill('time').fillna(1)\n",
    "    close_price_lwma = qnta.lwma(close_price, 10)\n",
    "    trend = qnta.roc(close_price_lwma, 1).ffill('time').bfill('time').fillna(1)\n",
    "    features = xr.concat([trend], \"feature\")\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_target_classes(data):\n",
    "    price_current = data.sel(field='open')\n",
    "    price_future = qnta.shift(price_current, -1)\n",
    "\n",
    "    class_positive = 1  # prices goes up\n",
    "    class_negative = 0  # price goes down\n",
    "\n",
    "    target_price_up = xr.where(price_future > price_current, class_positive, class_negative)\n",
    "    return target_price_up\n",
    "\n",
    "\n",
    "def get_dynamic_stock_data(data):\n",
    "    is_liquid = data.sel(field=\"is_liquid\")\n",
    "\n",
    "    # Get the assets that are liquid in the last time step\n",
    "    last_assets = is_liquid[-1]\n",
    "    is_liquid_asset_list = last_assets.where(last_assets > 0, drop=True).asset.values\n",
    "\n",
    "    # Select data for liquid assets\n",
    "    data_liquid = data.sel(asset=is_liquid_asset_list)\n",
    "\n",
    "    # Determine the rolling window for volatility calculation\n",
    "    rolling_window = min(global_lookback_period, len(data_liquid.time) - 1)\n",
    "\n",
    "    # Filter the top assets by lowest volatility\n",
    "    low_volatility = qnfilter.filter_volatility(\n",
    "        data=data_liquid,\n",
    "        rolling_window=rolling_window,\n",
    "        top_assets=global_top_assets,\n",
    "        metric=\"std\",\n",
    "        ascending=True\n",
    "    )\n",
    "\n",
    "    # Select data for the low volatility assets\n",
    "    last_asset = low_volatility[-1]\n",
    "    top_assets_indices = last_asset.where(last_asset > 0, drop=True).asset.values\n",
    "    data_low_volatility_assets = data.sel(asset=top_assets_indices)\n",
    "\n",
    "    return data_low_volatility_assets\n",
    "\n",
    "\n",
    "def load_data(period):\n",
    "    data = qndata.stocks.load_ndx_data(tail=period)\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "    data = get_dynamic_stock_data(data)\n",
    "    features_all = get_features(data)\n",
    "    target_all = get_target_classes(data)\n",
    "    models = dict()\n",
    "    asset_name_all = data.asset.values\n",
    "\n",
    "    for asset_name in asset_name_all:\n",
    "        model = get_model()\n",
    "        target_cur = target_all.sel(asset=asset_name).dropna('time', 'any')\n",
    "        features_cur = features_all.sel(asset=asset_name).dropna('time', 'any')\n",
    "        target_for_learn_df, feature_for_learn_df = xr.align(target_cur, features_cur, join='inner')\n",
    "        criterion = nn.MSELoss()\n",
    "        optimiser = optim.LBFGS(model.parameters(), lr=0.08)\n",
    "        epochs = 1\n",
    "        for i in range(epochs):\n",
    "            def closure():\n",
    "                optimiser.zero_grad()\n",
    "                feature_data = feature_for_learn_df.transpose('time', 'feature').values\n",
    "                in_ = torch.tensor(feature_data, dtype=torch.float32).unsqueeze(0)\n",
    "                out = model(in_)\n",
    "                target = torch.zeros(1, len(target_for_learn_df.values))\n",
    "                target[0, :] = torch.tensor(np.array(target_for_learn_df.values))\n",
    "                loss = criterion(out, target)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimiser.step(closure)\n",
    "        models[asset_name] = model\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict(models, data, state):\n",
    "    models_name = list(models.keys())\n",
    "    data = data.sel(asset=models_name)\n",
    "\n",
    "    features_all = get_features(data)\n",
    "\n",
    "    last_time = data.time.values[-1]\n",
    "    data_last = data.sel(time=slice(last_time, None))\n",
    "    features_last = features_all.sel(time=slice(last_time, None))\n",
    "\n",
    "    weights = xr.zeros_like(data_last.sel(field='close'))\n",
    "    for asset_name in models_name:\n",
    "        features_cur = features_last.sel(asset=asset_name).dropna('time', 'any')\n",
    "        if len(features_cur.time) < 1:\n",
    "            continue\n",
    "        feature_data = features_cur.transpose('time', 'feature').values\n",
    "        in_ = torch.tensor(feature_data, dtype=torch.float32).unsqueeze(0)\n",
    "        out = models[asset_name](in_)\n",
    "        prediction = out.detach()[0]\n",
    "        weights.loc[dict(asset=asset_name, time=features_cur.time.values)] = prediction\n",
    "\n",
    "    # Apply liquidity filter\n",
    "    weights = weights * data_last.sel(field=\"is_liquid\")\n",
    "\n",
    "    # state may be null, so define a default value\n",
    "    if state is None:\n",
    "        default = xr.zeros_like(data_last.sel(field='close')).isel(time=-1)\n",
    "        state = {\n",
    "            \"previous_weights\": default,\n",
    "        }\n",
    "\n",
    "    # Extract previous weights from state\n",
    "    if isinstance(state, tuple):\n",
    "        previous_weights = state[-1]['previous_weights']\n",
    "        # created = state[0]\n",
    "        # model = state[1]\n",
    "        # state = state[2]\n",
    "    else:\n",
    "        previous_weights = state['previous_weights']\n",
    "\n",
    "    # Combine the assets from both previous_weights and weights\n",
    "    # This code is needed because the list of liquid assets may change\n",
    "    all_assets = np.union1d(previous_weights.asset.values, weights.asset.values)\n",
    "    # Include new assets from weights in previous_weights, set to 0 if not present\n",
    "    weights = weights.reindex(asset=all_assets, fill_value=0)\n",
    "    previous_weights = previous_weights.reindex(asset=all_assets, fill_value=0)\n",
    "\n",
    "    # Ensure previous_weights includes the last time from weights\n",
    "    if last_time != previous_weights.time.values:\n",
    "        previous_weights = xr.concat([previous_weights, weights], dim='time')\n",
    "        previous_weights = previous_weights.reindex(asset=all_assets, fill_value=0)\n",
    "        # Shift previous weights by one time step to treat the past date as the current date, filling NaN values with 0\n",
    "        previous_weights = previous_weights.shift(time=1).fillna(0)\n",
    "        previous_weights = previous_weights.sel(time=last_time)\n",
    "\n",
    "    # Calculate average weights\n",
    "    weights_avg = xr.where(previous_weights == 0, weights, (weights + previous_weights) / 2)\n",
    "    weights_avg = xr.where(weights_avg < 0.005, 0, weights_avg)\n",
    "\n",
    "    # Normalize positions and cut big positions\n",
    "    weights_sum = abs(weights_avg).sum('asset')\n",
    "    weights_avg = xr.where(weights_sum > 1, weights_avg / weights_sum, weights_avg)\n",
    "    weights_avg = qnexp.cut_big_positions(weights=weights_avg, max_weight=0.1)\n",
    "\n",
    "    next_state = {\n",
    "        \"previous_weights\": weights_avg.isel(time=-1),\n",
    "    }\n",
    "    #\n",
    "    # print(last_time)\n",
    "    # print(\"previous_weights\")\n",
    "    # print(previous_weights)\n",
    "    # print(weights)\n",
    "    # print(\"weights_avg\")\n",
    "    # print(weights_avg.isel(time=-1))\n",
    "\n",
    "    return weights_avg, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-pass Version for Development and Testing Strategy\n",
    "\n",
    "**For a quick model check, we recommend starting with a short time period.\n",
    "Comment out this code before submitting the strategy to the competition.**\n",
    "\n",
    "```python\n",
    "weights = qnbt.backtest_ml(\n",
    "    load_data=load_data,\n",
    "    train=train_model,\n",
    "    predict=predict,\n",
    "    train_period=global_train_period,\n",
    "    retrain_interval=180,\n",
    "    retrain_interval_after_submit=1,\n",
    "    predict_each_day=True,\n",
    "    competition_type='stocks_nasdaq100',\n",
    "    lookback_period=global_lookback_period,\n",
    "    start_date='2020-12-30',\n",
    "    build_plots=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-pass Version for Participation in the Contest\n",
    "\n",
    "This code helps submissions get processed faster in the contest. The backtest system calculates the weights for each day, while the provided function calculates weights for only one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "from qnt.data import get_env\n",
    "from qnt.log import log_err, log_info\n",
    "\n",
    "\n",
    "def state_write(state, path=None):\n",
    "    if path is None:\n",
    "        path = get_env(\"OUT_STATE_PATH\", \"state.out.pickle.gz\")\n",
    "    try:\n",
    "        with gzip.open(path, 'wb') as gz:\n",
    "            pickle.dump(state, gz)\n",
    "        log_info(\"State saved: \" + str(state))\n",
    "    except Exception as e:\n",
    "        log_err(f\"Error saving state: {e}\")\n",
    "\n",
    "\n",
    "def state_read(path=None):\n",
    "    if path is None:\n",
    "        path = get_env(\"OUT_STATE_PATH\", \"state.out.pickle.gz\")\n",
    "    try:\n",
    "        with gzip.open(path, 'rb') as gz:\n",
    "            state = pickle.load(gz)\n",
    "        log_info(\"State loaded.\")\n",
    "        return state\n",
    "    except Exception as e:\n",
    "        log_err(f\"Can't load state: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "state = state_read()\n",
    "print(state)\n",
    "\n",
    "\n",
    "# separate cell\n",
    "\n",
    "def print_stats(data, weights):\n",
    "    stats = qns.calc_stat(data, weights)\n",
    "    display(stats.to_pandas().tail())\n",
    "    performance = stats.to_pandas()[\"equity\"]\n",
    "    qngraph.make_plot_filled(performance.index, performance, name=\"PnL (Equity)\", type=\"log\")\n",
    "\n",
    "\n",
    "data_train = load_data(global_train_period)\n",
    "models = train_model(data_train)\n",
    "\n",
    "data_predict = load_data(global_lookback_period)\n",
    "\n",
    "last_time = data_predict.time.values[-1]\n",
    "\n",
    "if last_time < np.datetime64('2006-01-02'):\n",
    "    print(\"The first state should be None\")\n",
    "    state_write(None)\n",
    "    state = state_read()\n",
    "    print(state)\n",
    "\n",
    "weights_predict, state_new = predict(models, data_predict, state)\n",
    "\n",
    "print_stats(data_predict, weights_predict)\n",
    "state_write(state_new)\n",
    "print(state_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qnout.write(weights_predict)  # To participate in the competition, save this code in a separate cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Strategy for Competitive Submissions\n",
    "\n",
    "To enhance your machine learning-based strategy for competitive submissions, consider the following guidelines tailored for efficiency and robustness:\n",
    "\n",
    "### Model Retraining Frequency\n",
    "- Your configuration to retrain the model daily (`retrain_interval_after_submit=1`) after competition submission is noted. For a more streamlined approach, adjust your strategy to a single-pass mode, conducive to the competition's environment. Utilize the available [precheck](https://github.com/quantiacs/toolbox/blob/main/qnt/precheck.ipynb) feature for a preliminary quality assessment of your model.\n",
    "\n",
    "### Acceleration Techniques\n",
    "To expedite the development process, you might explore:\n",
    "- **Model Simplification**: Opt for less complex machine learning models to reduce computational demands.\n",
    "- **Local Development Enhancements**: Utilize a high-performance computer locally or deploy your script on a potent server for accelerated computations.\n",
    "- **Data Volume Reduction**: Limit the dataset size to hasten model training and evaluation.\n",
    "- **Condensed Testing Phases**: Shorten the evaluation timeframe by focusing on recent performance metrics, such as examining the model's financial outcomes over the past year.\n",
    "\n",
    "### Data Preparation and Feature Engineering\n",
    "- **Pre-calculated Indicators**: Employ pre-calculated technical indicators like Exponential Moving Averages (EMA) to enrich your features without the risk of lookahead bias. Example: `g_ema = qnta.ema(data_all.sel(field=\"high\"), 15)` ensures indicators are prepared ahead of the model training phase.\n",
    "\n",
    "### Other Topics\n",
    "- [Backtest ML has too long a run time](https://quantiacs.com/community/topic/528/backtest_ml-has-too-long-a-run-time/3)\n",
    "- [Printing training performance of neural network models](https://quantiacs.com/community/topic/537/printing-training-performance-of-neural-network-models)\n",
    "- [Acess previous weights](https://quantiacs.com/community/topic/555/acess-previous-weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
